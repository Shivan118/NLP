{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shashi.singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shashi.singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shashi.singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk # import package for tokenization\n",
    "nltk.download('punkt') # download all spporting function /files for NLTK package\n",
    "nltk.download('wordnet') # download all spporting function /files for NLTK package\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming <br>\n",
    "Stemming algorithm works by cutting the suffix from the word. In a broader sense cuts either the beginning or end of the word.\n",
    "1. Porter Stemmer\n",
    "2. Lancaster Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go: go\n",
      "goes: goe\n",
      "went: went\n",
      "gone: gone\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "poster=PorterStemmer()\n",
    "# Try some stems\n",
    "print('go: {}'.format(poster.stem('go')))\n",
    "print('goes: {}'.format(poster.stem('goes')))\n",
    "print('went: {}'.format(poster.stem('went')))\n",
    "print('gone: {}'.format(poster.stem('gone')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go: go\n",
      "goes: goe\n",
      "went: went\n",
      "gone: gon\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "# Try some stems\n",
    "print('go: {}'.format(lancaster.stem('go')))\n",
    "print('goes: {}'.format(lancaster.stem('goes')))\n",
    "print('went: {}'.format(lancaster.stem('went')))\n",
    "print('gone: {}'.format(lancaster.stem('gone')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Lemmatization? <br> \n",
    "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as the lemma. The NLTK Lemmatization method is based on WorldNet's built-in morph function. Text preprocessing includes both stemming as well as lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go: go\n",
      "goes: go\n",
      "went: went\n",
      "gone: gone\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "from nltk.stem import WordNetLemmatizer \n",
    " # Initializing WordNetLemmatizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print('go: {}'.format(lemmatizer.lemmatize('go')))\n",
    "print('goes: {}'.format(lemmatizer.lemmatize('goes')))\n",
    "print('went: {}'.format(lemmatizer.lemmatize('went')))\n",
    "print('gone: {}'.format(lemmatizer.lemmatize('gone')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet <b>\n",
    "Wordnet is an large, freely and publicly available lexical database for the English language aiming to establish\n",
    "structured semantic relationships between words. It offers lemmatization capabilities as well and is one of the \n",
    "earliest and most commonly used lemmatizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {'J': 'a', 'V': 'v', 'R': 'r'})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('tagsets')\n",
    "#nltk.help.upenn_tagset()# tagset documentation\n",
    "#nltk.download('wordnet')\n",
    "from collections import defaultdict #Default Dictionary is imported from collections\n",
    "from nltk.corpus import wordnet as wn #the corpus reader wordnet is imported.\n",
    "from nltk.tag import pos_tag\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. \n",
    "#By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN) #Dictionary is created where pos_tag (first letter) are the key values \n",
    "tag_map['J'] = wn.ADJ                   #whose values are mapped with the value \n",
    "tag_map['V'] = wn.VERB                  #from wordnet dictionary. We have taken the only first letter as \n",
    "tag_map['R'] = wn.ADV\n",
    "# we will use it later in the loop.\n",
    "tag_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go: go\n",
      "goes: go\n",
      "went: go\n",
      "gone: go\n",
      "goes: go\n",
      "goes: go\n",
      "goes: goes\n",
      "goes: goes\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "from nltk.stem import WordNetLemmatizer \n",
    " # Initializing WordNetLemmatizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print('go: {}'.format(lemmatizer.lemmatize('go',tag_map[tag[0]])))\n",
    "print('goes: {}'.format(lemmatizer.lemmatize('goes',tag_map[tag[0]])))\n",
    "print('went: {}'.format(lemmatizer.lemmatize('went',tag_map[tag[0]])))\n",
    "print('gone: {}'.format(lemmatizer.lemmatize('gone',tag_map[tag[0]])))\n",
    "print('goes: {}'.format(lemmatizer.lemmatize('goes','v')))\n",
    "print('goes: {}'.format(lemmatizer.lemmatize('goes','n')))\n",
    "print('goes: {}'.format(lemmatizer.lemmatize('goes','r')))\n",
    "print('goes: {}'.format(lemmatizer.lemmatize('goes','a')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is Lemmatization better than Stemming? <br>\n",
    "\n",
    "Stemming algorithm works by cutting the suffix from the word. In a broader sense cuts either the beginning or end of the word.\n",
    "\n",
    "On the contrary, Lemmatization is a more powerful operation, and it takes into consideration morphological analysis of the words. It returns the lemma which is the base form of all its inflectional forms. In-depth linguistic knowledge is required to create dictionaries and look for the proper form of the word. Stemming is a general operation while lemmatization is an intelligent operation where the proper form will be looked in the dictionary. Hence, lemmatization helps in forming better machine learning features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                  Poster Stemmer        Lancaster Stemmer     Lemmatization W/O POS Lemmatization with POS\n",
      "---------------       ---------------       ---------------       ---------------       ---------------       \n",
      "detections            detect                detect                detection             detection             \n",
      "detected              detect                detect                detected              detect                \n",
      "detection             detect                detect                detection             detection             \n",
      "detecting             detect                detect                detecting             detect                \n",
      "go                    go                    go                    go                    go                    \n",
      "went                  went                  went                  went                  go                    \n",
      "gone                  gone                  gon                   gone                  go                    \n",
      "going                 go                    going                 going                 go                    \n",
      "goa                   goa                   goa                   goa                   goa                   \n",
      "send                  send                  send                  send                  send                  \n",
      "sent                  sent                  sent                  sent                  sent                  \n",
      "sending               send                  send                  sending               send                  \n",
      "console               consol                consol                console               console               \n",
      "consoling             consol                consol                consoling             console               \n",
      "run                   run                   run                   run                   run                   \n",
      "ran                   ran                   ran                   ran                   run                   \n",
      "running               run                   run                   running               run                   \n",
      "\n",
      " It returned going as such without converting it to the root form go. This is because the lemmatization process depends on the POS tag to come up with the correct lemma. Now let us lemmatize again by providing the POS tag for the word.\n"
     ]
    }
   ],
   "source": [
    "#Poster Stemmer VS Lancaster Stemmer VS Lemmatization\n",
    "\n",
    "word_list = [\"---------------\",\"detections\",\"detected\",\"detection\",\"detecting\",\"go\",\"went\",\"gone\",\"going\",\"goa\",\"send\",\"sent\",\"sending\",\"console\",\"consoling\",\"run\",\"ran\",\"running\"]\n",
    "print(\"{0:22}{1:22}{2:22}{3:22}{4:22}\".format(\"Word\",\"Poster Stemmer\",\"Lancaster Stemmer\",\"Lemmatization W/O POS\",\"Lemmatization with POS\"))\n",
    "for word,tag in pos_tag(word_list):\n",
    "    print(\"{0:22}{1:22}{2:22}{3:22}{4:22}\".format(word,poster.stem(word),lancaster.stem(word),lemmatizer.lemmatize(word),lemmatizer.lemmatize(word,tag_map[tag[0]])))\n",
    "print(\"\")\n",
    "print(\" It returned going as such without converting it to the root form go. This is because the lemmatization process depends on the POS tag to come up with the correct lemma.\", \n",
    " \"Now let us lemmatize again by providing the POS tag for the word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text                  Lemma                 POS                   tag                   \n",
      "detections            detection             NOUN                  NNS                   \n",
      "detected              detect                VERB                  VBD                   \n",
      "detection             detection             NOUN                  NN                    \n",
      "detecting             detect                VERB                  VBG                   \n",
      "go                    go                    VERB                  VB                    \n",
      "went                  go                    VERB                  VBD                   \n",
      "gone                  go                    VERB                  VBN                   \n",
      "going                 go                    VERB                  VBG                   \n",
      "goa                   goa                   PROPN                 NNP                   \n",
      "send                  send                  VERB                  VB                    \n",
      "sent                  send                  VERB                  VBD                   \n",
      "sending               send                  VERB                  VBG                   \n",
      "console               console               NOUN                  NN                    \n",
      "consoling             consoling             NOUN                  NN                    \n",
      "run                   run                   NOUN                  NN                    \n",
      "ran                   run                   VERB                  VBD                   \n",
      "running               run                   VERB                  VBG                   \n"
     ]
    }
   ],
   "source": [
    "#Poster Stemmer VS Lancaster Stemmer VS Lemmatization\n",
    "\n",
    "word_list = (\"detections detected detection detecting go went gone going goa send sent sending console consoling run ran running\")\n",
    "#word_list=(\"My name is Shaurya Uppal.  I enjoy writing articles on GeeksforGeeks checkout my other article by going to my profile section.\")\n",
    "dd=nlp(word_list)\n",
    "print(\"{0:22}{1:22}{2:22}{3:22}\".format(\"Text\", \"Lemma\",\"POS\", \"tag\"))\n",
    "for token in dd:\n",
    "  print(\"{0:22}{1:22}{2:22}{3:22}\".format(token.text, token.lemma_, token.pos_, token.tag_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:-\n",
    "1.https://www.guru99.com/stemming-lemmatization-python-nltk.html\n",
    "2.https://www.geeksforgeeks.org/python-pos-tagging-and-lemmatization-using-spacy/\n",
    "3.https://www.nltk.org/  \n",
    "4.https://spacy.io/\n",
    "5.https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "6.https://wordnet.princeton.edu/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
